# Recompose Samples

The `recompose` samples exercise MiniPhi's code⇄markdown benchmarking flow. Each sample folder contains:

- `code/`: canonical source files.
- `descriptions/`: markdown representations of the files (generated via `code-to-markdown`).
- `reconstructed/`: output directory for `markdown-to-code` (created on demand).
- `recompose-report.json`: optional benchmark summaries generated by the CLI.

Quick start:

```bash
node src/index.js recompose --sample samples/recompose/hello-flow --direction roundtrip --clean
```

This command wipes previous markdown/code outputs, converts each file under `code/` into markdown natural language descriptions without code snippets, rebuilds code from the markdown, compares the round-trip fidelity, and stores a step-by-step report alongside the sample.

## Narrative-only descriptions and Phi-4 prompts

`code-to-markdown` no longer dumps source verbatim. MiniPhi now orchestrates a chain of Phi-4 prompts that (1) scans the workspace to build a story-driven overview saved under `.miniphi/recompose/<timestamp>/workspace-overview.md`, and (2) rewrites each file as multi-section prose stored in `descriptions/` plus the `.miniphi` session history. Every description must explain behavior entirely in narrative form—no fenced code, no line-by-line mirroring—so recomposition requires genuine reasoning. During `markdown-to-code`, the CLI issues additional Phi-4 prompts to plan each file and then synthesize the source from the plan. All sub-prompts are captured in `.miniphi/prompt-exchanges/` to prove the benchmark exercised multi-step local inference with `microsoft/phi-4-reasoning-plus`. Running any recompose command with `--verbose` also writes a human-readable prompt log under `.miniphi/recompose/<timestamp>/prompts.log` so teams can audit the full sequence afterward.

## Timestamped benchmark batches

Use the dedicated helper to create WHY_SAMPLES-friendly runs without hand-rolling timestamps:

```bash
node src/index.js benchmark recompose --directions roundtrip,code-to-markdown,markdown-to-code --repeat 2 --clean
```

Each invocation writes `RUN-###.{json,log}` pairs under `samples/benchmark/recompose/<dd-mm-yy_mm-hh>/`. Summaries for a given batch now include `SUMMARY.json`, `SUMMARY.md`, and `SUMMARY.html` to make it easy to embed the stats inside docs/PRs:

```bash
node src/index.js benchmark analyze samples/benchmark/recompose/<dd-mm-yy_mm-hh>
```

## Plan-driven benchmark series

For repeatable sweeps, feed a JSON/YAML plan to the `benchmark recompose` helper. The repo ships with `samples/recompose/hello-flow/benchmark-plan.yaml` which keeps the sample directory relative, pins the timestamp folder, and shows how to toggle `clean` and `runPrefix` per run:

```bash
node src/index.js benchmark recompose --plan samples/recompose/hello-flow/benchmark-plan.yaml
```

Plan entries accept `directions`, `repeat`, `clean`, `runPrefix`, and optional `label` fields so you can mix round-trips, direction-only passes, or clean-only reruns without touching CLI flags.
